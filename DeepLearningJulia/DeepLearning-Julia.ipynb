{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: page\n",
    "title: \"Deep Learning with Julia\"\n",
    "permalink: /deeplearning-julia/\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a specialized subset of machine learning that uses multi-layered neural networks (often called deep neural networks) to learn complex patterns from data​. In this tutorial, we'll introduce **Flux.jl**, a pure-Julia library for machine learning models (including deep neural networks)​, and walk through building, training, and evaluating a neural network on the classic MNIST dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial covers**:\n",
    "\n",
    "1. Environment setup and data preprocessing\n",
    "\n",
    "2. Building a simple neural network model\n",
    "\n",
    "3. Training the model with mini-batching\n",
    "\n",
    "4. Evaluating the model and plotting results\n",
    "\n",
    "5. Advanced topics: checkpointing and mixed-precision training with an FP32 master copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard ML workflow**: \n",
    "\n",
    "- **Data Collection & Preparation**: MNIST provides images of handwritten digits along with their true labels (0 through 9)\n",
    "\n",
    "- **Model Building**: Neural network (a chain of layers) that will take inputs (images) and produce outputs (predicted labels)\n",
    "\n",
    "- **Training**: Forward Pass (using a loss function), and adjust the model's parameters (weights) to reduce this error (iterative gradient descent)\n",
    "\n",
    "- **Evaluation**: Test set Evaluation on model's performance (e.g., accuracy in classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Flux.jl\n",
    "[Flux.jl](https://fluxml.ai/Flux.jl/stable/) is an open-source machine learning library written solely in Julia. It provides a flexible and maths-friendly framework for building neural networks (e.g. feed-forward networks, convolutional networks, recurrent networks, etc.) in just a few lines of code, very similar to PyTorch. It integrates smoothly with the Julia language and has noticeable features such as automatic differentiation (AD) and CUDA support. \n",
    "\n",
    "Some key characteristics of Flux.jl:\n",
    "\n",
    "- **Easy Model Definition**: You can define models like writing simple Julia functions. Flux provides layers like ```Dense``` (fully connected layer), ```Conv``` (convolutional layer), activation functions (like ```relu```, ```sigmoid```), and utilities to chain them together. (more example to come)\n",
    "\n",
    "- **Gradients and Training**: Flux handles backpropagation (gradient calculation) for you via Julia's automatic differentiation (AD) system. You just define a loss function, and Flux can compute gradients and update parameters using optimizers (like SGD, Adam).\n",
    "\n",
    "- **Integration with Julia Ecosystem**: Rather than being a monolithic framework, Flux works with other Julia packages. For example, it uses MLDatasets.jl for easy data loading (which we'll use for MNIST) and allows using any Julia array type (CPU or GPU arrays, etc.) seamlessly​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set Up\n",
    "We’ll use **Flux** for deep learning, **CUDA** for GPU support, and **Plots** for visualization.  We'll also use the **MLDatasets** which contains common datasets including MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.add(\"Flux\")       \n",
    "# Pkg.add(\"MLDatasets\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux            \n",
    "using MLDatasets      \n",
    "\n",
    "# MNIST dataset\n",
    "train_x, train_y = MLDatasets.MNIST.traindata()  \n",
    "test_x,  test_y  = MLDatasets.MNIST.testdata()   \n",
    "\n",
    "println(size(train_x))   # Expect (28, 28, 60000) - 60k images of 28x28 pixels\n",
    "println(size(train_y))   # Expect (60000,) - 60k labels corresponding to the images\n",
    "println(size(test_x))    # Expect (28, 28, 10000) - 10k test images\n",
    "println(size(test_y))    # Expect (10000,) - 10k test labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Minist.png\" alt=\"Loss Curve\" style=\"max-width:100%; width:600px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: The MNIST dataset consists of grayscale images of handwritten digits 0 through 9, each image being 28x28 pixels​. The code above loads ```train_x``` and ```train_y``` as the training set images and labels, and similarly ```test_x``` and ```test_y``` for the test set. According to the dataset, there are 60,000 training examples and 10,000 test examples​, which matches the printed shapes:\n",
    "\n",
    "- ```train_x``` is a 28×28×60000 array (the third dimension indexes the images).\n",
    "\n",
    "- ```train_y``` is a vector of length 60000, containing the digit labels (0–9) for each training image.\n",
    "\n",
    "- The test set shapes are 28×28×10000 for images and 10000 for labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing step**: in this case, ```train_x``` and ```test_x``` might be of type ```N0f8``` (normalized 8-bit fixed-point) or ```UInt8``` representing pixel intensities. We'll convert them to ```Float32``` for Flux, and also normalize pixel values to the 0-1 range if they aren't already. (Often, ```MLDatasets``` provides the images already as Float32 in [0,1].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Encoding**: Tune labels for multi-class classification​. It means representing each label as a vector of length 10 (for digits 0-9) that has a 1 in the position corresponding to the digit and 0 in all other positions. For example, label ```3``` becomes ```[0,0,0,1,0,0,0,0,0,0]```. Flux provides a convenient ```onehotbatch``` function to do this conversion for a batch of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = Float32.(train_x)  \n",
    "test_x  = Float32.(test_x)\n",
    "\n",
    "# One-hot encode\n",
    "train_y_onehot = Flux.onehotbatch(train_y, 0:9)  # 10×60000 matrix of one-hot columns\n",
    "test_y_onehot  = Flux.onehotbatch(test_y, 0:9)   # 10×10000 one-hot encoded labels for test\n",
    "\n",
    "println(train_y[1], \" -> \", vec(train_y_onehot[:,1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"OneHot.png\" alt=\"My Plot\" style=\"max-width:100%; width:800px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: We used ```Float32.(train_x)``` to broadcast conversion of each element to ```Float32```. \n",
    "\n",
    "If the original pixel values were 0-255, converting to ```Float32``` will yield 0.0-255.0; if they were ```N0f8``` (0.0-1.0 in an 8-bit format), the conversion yields 0.0-1.0 floats. In either case, a neural network can work with these scaled inputs (though if it were 0-255, we might explicitly divide by 255 to scale to [0,1]). This may be an issue when coming into lower precision such as ```Float16``` as we will discuss later. \n",
    "\n",
    "One-hot encoding is done via ```Flux.onehotbatch(labels, 0:9)```, which produces a 10-row matrix where each column is a one-hot representation of the corresponding label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move to GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA  \n",
    "if CUDA.has_cuda()\n",
    "    println(\"CUDA is available! Using GPU for computations.\")\n",
    "    device(x) = cu(x)  # GPU\n",
    "else\n",
    "    println(\"CUDA not available. Using CPU.\")\n",
    "    device(x) = x      # CPU\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_gpu = device(train_x);\n",
    "train_y_gpu = device(train_y);\n",
    "test_x_gpu = device(test_x);\n",
    "test_y_gpu = device(test_y);\n",
    "train_y_onehot_gpu = device(train_y_onehot);\n",
    "test_y_onehot_gpu = device(test_y_onehot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note: \n",
    "\n",
    "```cu(x)``` is a generic, higher-level conversion function that “adapts” its input to a GPU array. If ```x``` is already a ```CuArray```, it simply returns it unchanged. And if ```x``` is a CPU array, it's equivalent to calling ```CuArray(x)```\n",
    "\n",
    "```CuArray(x)``` explicit construct GPU arrays which make it transparent for speed / memory demostration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network Model with Flux\n",
    "Flux makes it easy to define neural network models —similar in spirit to the ```custom nn.Module``` example in PyTorch - using the ```Chain``` function, which combines layers (and functions) sequentially. A neural network can be thought of as a chain of layers, where each layer transforms its input to some output; these outputs become inputs to the next layer​. In Flux, ```Chain``` takes a list of layer constructors (or functions) and creates a callable model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our MNIST classifier, we'll build a simple feed-forward neural network (multi-layer perceptron) with one hidden layer:\n",
    "\n",
    "- **Input layer**: 28×28 pixels per image, which we will **flatten** into a 784-dimensional vector (this can be done with ```Flux.flatten``` as a layer in the chain)\n",
    "\n",
    "- **Hidden layer**: a fully connected dense layer with, say, 128 neurons and a ReLU activation. This layer will take the 784-dim input and produce 128 outputs.\n",
    "\n",
    "- **Output layer**: a dense layer with 10 neurons (one for each digit class) producing the raw scores for each class. We'll later apply a softmax or appropriate loss to these scores to get class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: Dense, Chain, relu, flatten  \n",
    "\n",
    "model = Chain(\n",
    "    flatten,                 # flatten 28x28 input images into 784-element vectors\n",
    "    Dense(28*28, 128, relu), # hidden layer: 784 -> 128, with ReLU activation\n",
    "    Dense(128, 10)           # output layer: 128 -> 10 (raw scores for 10 digits)\n",
    ")\n",
    "\n",
    "model_gpu = device(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used ```Chain``` to stack three components: ```flatten``` (to convert 2D image to 1D vector), a ```Dense``` layer with 128 neurons and ReLU, and another ```Dense``` layer with 10 outputs. The ```Dense(input_dim, output_dim, activation)``` constructor creates a fully connected layer (it automatically initializes weights and biases). Here, the first Dense layer takes 784 inputs and gives 128 outputs with ReLU activation applied, and the second Dense layer takes 128 inputs and gives 10 outputs. We did not specify an activation for the output layer; in classification tasks, it's common to apply a softmax at the end to interpret outputs as probabilities​, but we'll handle that in the loss function for numerical stability instead of as a separate layer. (Alternatively, one could add ```softmax``` as the final layer in the Chain, but then one should use a corresponding loss that expects probabilities.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in [Notes 8 GPUs](https://stat244.berkeley.edu/spring-2025/notes/notes8.html), we could write custom CUDA kernels in Julia to accelerate operations in training loop. Given that most operations (matrix multiples, convolutions, etc.) are highly optimized using libraries like cuDNN already, one common target is the **activation function**. A simple demo in Appendix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ```model``` is a Flux model that we can call like a function. For example, ```model(train_x[:,:,1:5])``` would output predictions for the first 5 images (though they would be untrained, random predictions initially). We haven't trained the model yet, so if we tried to predict, the outputs would be basically random relative to the true labels​. Training will adjust the weights to make these predictions meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU & CPU Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time cpu_predictions = model(test_x)\n",
    "# 2.668480 seconds (2.98 M allocations: 211.474 MiB, 79.97% compilation time)\n",
    "# 0.610637 seconds (10 allocations: 10.529 MiB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time gpu_predictions = model_gpu(test_x_gpu)\n",
    "# 3.151836 seconds (2.50 M allocations: 166.681 MiB, 2.00% gc time, 89.35% compilation time)\n",
    "# 0.000341 seconds (298 allocations: 6.703 KiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Boost with Lower Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication with Float64\n",
    "A64 = rand(10000, 10000)\n",
    "B64 = rand(10000, 10000)\n",
    "@time C64 = A64 * B64\n",
    "# 25.654823 seconds (2 allocations: 762.939 MiB, 0.27% gc time)\n",
    "\n",
    "# Matrix multiplication with Float32\n",
    "A32 = rand(Float32, 10000, 10000)\n",
    "B32 = rand(Float32, 10000, 10000)\n",
    "@time C32 = A32 * B32\n",
    "# 13.297140 seconds (2 allocations: 381.470 MiB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux leverages [Zygote](https://fluxml.ai/Zygote.jl/dev/) for source-to-source automatic differentiation (AD), which means that the ```gradient(f, x)``` connects to Julia's compiler to transform operations in ```f``` to produce code for computing ```∂f/∂x```. Whether you work with implicit global parameters or pass them explicitly, [Flux](https://fluxml.ai/Flux.jl/stable/guide/models/basics/) uses AD to compute gradients efficiently.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Implicit Parameters with Global Variables\n",
    "\n",
    "Defines a polynomial using a closure over the global vector ```θ```. When we call ```gradient(poly, 5.0)```, Flux computes the derivative of ```poly``` at 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to input: 2.0\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "θ = [10.0, 1.0, 0.1] # Global parameters for a quadratic polynomial\n",
    "\n",
    "poly(x) = θ[1] + θ[2]*x + θ[3]*x^2\n",
    "\n",
    "# Gradient of poly with respect to input x at x = 5\n",
    "grad_input = gradient(poly, 5.0) # d(poly)/dx.\n",
    "println(\"Gradient with respect to input: \", grad_input[1])\n",
    "# d/dx (10 + x + 0.1*x^2) = 1 + 0.2*x, and at x=5, 1+1=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Explicit Parameter Passing\n",
    "\n",
    "Defines ```poly2``` where parameters are explicitly passed. This allows Flux to return a tuple with gradients with respect to both the input and the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to input: 2.0\n",
      "Gradient with respect to x: 2.0\n",
      "Gradient with respect to parameters: [1.0, 5.0, 25.0]\n"
     ]
    }
   ],
   "source": [
    "# Alternative version that takes parameters explicitly.\n",
    "poly2(x, θ2) = evalpoly(x, θ2) # built-in, from Base.Math\n",
    "\n",
    "# Gradients with respect to both input and parameters.\n",
    "grad_input_param = gradient(poly2, 5.0, θ)\n",
    "println(\"Gradient with respect to x: \", grad_input_param[1])\n",
    "println(\"Gradient with respect to parameters: \", grad_input_param[2])\n",
    "# Grad_input_param[2] is a vector of derivatives [∂poly2/∂θ[1], ∂poly2/∂θ[2], ∂poly2/∂θ[3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Gradient``` is also used within ```train!``` to differentiate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss Function and Optimizer\n",
    "**Loss Function**: For a multi-class classification problem like MNIST, a common choice of loss function is *cross-entropy loss*. Cross-entropy measures the difference between two probability distributions: in our case, the model's predicted distribution over classes vs. the true distribution (which for a correct label is a one-hot vector)​. We will use Flux's built-in cross-entropy implementations. Specifically, we will use ```Flux.Losses.logitcrossentropy```, which expects the model's **raw output scores (logits)** and the true one-hot vector, and internally applies the softmax and cross-entropy in a numerically stable way. This is equivalent to applying a softmax to get probabilities and then using crossentropy, but ```logitcrossentropy``` is preferred to avoid potential numerical issues. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**: We need to choose an optimization algorithm to update the model's weights based on the gradients of the loss. A good default for neural networks is **Adam** (Adaptive Moment Estimation), which often converges faster than basic stochastic gradient descent​. We'll use Flux's **ADAM optimizer** with a modest learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to gather the model parameters into a container that the optimizer will update. Flux provides ```params(model)``` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: onecold  \n",
    "\n",
    "loss_function(x, y) = Flux.Losses.logitcrossentropy(model(x), y)  \n",
    "\n",
    "opt = Flux.Optimise.ADAM(0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# Trainable parameters\n",
    "parameters = Flux.params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick breakdown:\n",
    "\n",
    "- ```loss_function(x, y)``` runs our ```model``` on input ```x``` and compares the output to true one-hot label ```y``` using logit cross-entropy. This will give a scalar loss value (or average loss over a batch).\n",
    "\n",
    "- ```opt = ADAM(0.001)``` creates an Adam optimizer. (Accessed via ```Flux.Optimise.ADAM``` – note some Flux versions you might use ```ADAM()``` if properly imported. The exact namespace isn't too important, as long as we have an ```opt``` object.)\n",
    "\n",
    "- ```parameters = Flux.params(model)``` collects all the weight and bias arrays from our model layers. Flux will use this to know what values to update during training.\n",
    "\n",
    "\n",
    "Now we have all components ready for training: the model (```model```), the objective (```loss_function```), the data (```train_x``` and ```train_y_onehot```), and the optimizer (```opt```)​. We can proceed to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Training a model in Flux typically involves iterating over the dataset multiple times (epochs) and updating the model parameters to gradually reduce the loss​. We have a few options for how to implement the training loop:\n",
    "\n",
    "- Use ```Flux.train!```, a convenience function that automates the loop over data points or batches\n",
    "\n",
    "- Manually write a loop using ```Flux.gradient``` and update the parameters (Flux allows that, but it's more verbose).\n",
    "\n",
    "- Use mini-batches for efficiency (especially for large datasets) via ```Flux.Data.DataLoader``` to batch and shuffle data​\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll use ```train!``` along with a DataLoader for mini-batching. This way, we can train in batches (say 128 images at a time) rather than one image at a time or the entire dataset at once, compromising speed and stability.\n",
    "\n",
    "Let's create a DataLoader for our training data and run a training loop for a certain number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Data: DataLoader\n",
    "using Plots\n",
    "\n",
    "# Data loader for mini-batch iteration\n",
    "batch_size = 128\n",
    "train_loader = DataLoader((train_x, train_y_onehot), batchsize=batch_size, shuffle=true)\n",
    "\n",
    "train_losses = Float32[]\n",
    "\n",
    "epochs = 5 \n",
    "for epoch in 1:epochs\n",
    "    for (x_batch, y_batch) in train_loader\n",
    "        Flux.train!(loss_function, parameters, [(x_batch, y_batch)], opt)\n",
    "    end\n",
    "    train_loss = loss_function(train_x[:, :, 1:1000], train_y_onehot[:, 1:1000])  # loss on a subset\n",
    "    push!(train_losses, train_loss)\n",
    "    println(\"Epoch $epoch complete. Sample training loss = $(train_loss).\")\n",
    "end\n",
    "\n",
    "p1 = plot(train_losses, \n",
    "          title=\"Training Loss\", \n",
    "          label=\"Loss\", \n",
    "          xlabel=\"Epoch\", \n",
    "          ylabel=\"Loss\", \n",
    "          linewidth=2, \n",
    "          marker=:circle)\n",
    "savefig(p1, \"loss_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss_plot.png\" alt=\"Loss Curve\" style=\"max-width:100%; width:600px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: We constructed ```train_loader``` by passing a tuple ```(train_x, train_y_onehot)``` to ```DataLoader```, with a batch size of 128 and ```shuffle=true``` to randomize order each epoch. In the training loop, for each epoch we loop over ```train_loader```, which yields batches ```(x_batch, y_batch)``` of size 128. We then call ```Flux.train!(loss_function, parameters, [(x_batch, y_batch)], opt)``` to perform parameter updates for that batch. The ```train!``` function will compute the gradient of ```loss_function(x_batch, y_batch)``` with respect to ```parameters``` and update them using the Adam optimizer​.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print sample training loss on a subset of data after each epoch. As the epochs progress, we observe the loss decreasing, indicating that the model is learning. \n",
    "\n",
    "**Note**: With 5 epochs and a batch size of 128 on 60k examples, that's about 5 * (60000/128) ≈ 5 * 469 batches, which is quite manageable on CPU for a small network and even faster with GPU (CuArrays). \n",
    "\n",
    "**Note**: Training process above is its **in-place** modification of the model parameters, as demonstrated by the successive execution of two training phases: an initial run with 5 epochs followed by a subsequent run with 10 epochs. Flux performs parameter updates directly on the existing parameters object, accessed via ```Flux.params(model)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "After training, we should assess how well the model generalizes to unseen data — in this case, the 10,000 images in our test set (which were not used for training). We'll use the trained model to predict labels for the test images and then compute the accuracy: the fraction of images for which the predicted label matches the true label.We have a few ways to get predictions:\n",
    "\n",
    "- We could get the raw scores from the model and take the index of the highest score (since the highest logit corresponds to the most likely class).\n",
    "\n",
    "- Since we one-hot encoded the test labels, we can also compare one-hot predictions to the true one-hot vectors.\n",
    "\n",
    "Flux provides a utility ```onecold``` which is the inverse of one-hot encoding — it can take the model's probability output (or logits) and return the predicted class labels​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logits = model(test_x)                       # outputs (logits) for each test image\n",
    "predicted_labels = onecold(y_pred_logits, 0:9)      # digit (0-9)\n",
    "\n",
    "true_labels = test_y\n",
    "\n",
    "accuracy = sum(predicted_labels .== true_labels) / length(true_labels)\n",
    "println(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ```model(test_x)``` produces a 10×10000 matrix of raw scores (each column corresponds to one test image). The ```onecold(..., 0:9)``` function finds the index of the largest score in each column and maps it to the corresponding label in ```0:9```​. This gives a vector of 10000 predicted digit labels. We then compare this to the true labels and compute the proportion that are equal. The result is the accuracy (a number between 0 and 1, where 1.0 would mean 100% correct).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpointing (Saving and Loading)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long training runs should save model checkpoints periodically (e.g., every few epochs) to guard against crashes and to enable later analysis or fine-tuning. Rather than saving the entire model (which can lead to compatibility issues over time), a recommended approach is to save only the model’s parameters. In our example, we use the BSON format to store a CPU copy of the model’s parameters along with the current epoch. This is achieved by converting the model parameters to CPU-friendly Float32 arrays using ```cpu.(Flux.params(model))```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later,to resume training or fine-tune the model, we reload the checkpoint using BSON, move the parameters back to the GPU (if available) with ```gpu.(model_params)```, and then update the model with ```Flux.loadparams!(model, model_params)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using CUDA\n",
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Flux.Optimise: ADAM\n",
    "using BSON\n",
    "using Plots\n",
    "using Flux: Dense, Chain, relu, flatten, gpu, fmap\n",
    "\n",
    "# Utility: move to GPU if available\n",
    "device(x) = CUDA.has_cuda() ? cu(x) : x\n",
    "\n",
    "# Data loading and preprocessing (used in both training and reloading)\n",
    "function load_data()\n",
    "    # Load MNIST data\n",
    "    train_x, train_y = MLDatasets.MNIST.traindata()\n",
    "    test_x,  test_y  = MLDatasets.MNIST.testdata()\n",
    "\n",
    "    # Convert images to Float32 and send to device (GPU if available)\n",
    "    train_x = device(Float32.(train_x))\n",
    "    test_x  = device(Float32.(test_x))\n",
    "\n",
    "    # One-hot encode labels and send to device\n",
    "    train_y_onehot = device(Flux.onehotbatch(train_y, 0:9))\n",
    "    test_y_onehot  = device(Flux.onehotbatch(test_y, 0:9))\n",
    "\n",
    "    return train_x, train_y_onehot, test_x, test_y_onehot\n",
    "end\n",
    "\n",
    "# Build the model and send it to the appropriate device\n",
    "function build_model()\n",
    "    model = Chain(\n",
    "        flatten,\n",
    "        Dense(28*28, 128, relu),\n",
    "        Dense(128, 10)\n",
    "    )\n",
    "    return device(model)\n",
    "end\n",
    "\n",
    "# Define the loss function\n",
    "loss_function(model, x, y) = logitcrossentropy(model(x), y)\n",
    "\n",
    "# Training loop with optional checkpointing\n",
    "function train_model!(model, train_x, train_y; \n",
    "                      epochs=5, batch_size=128, \n",
    "                      checkpoint_interval=5, checkpoint_prefix=\"checkpoint_epoch\")\n",
    "    opt = ADAM(0.0001)\n",
    "    params = Flux.params(model)\n",
    "    train_loader = DataLoader((train_x, train_y), batchsize=batch_size, shuffle=true)\n",
    "    current_epoch = 0\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        for (x_batch, y_batch) in train_loader\n",
    "            grads = gradient(() -> loss_function(model, x_batch, y_batch), params)\n",
    "            Flux.Optimise.update!(opt, params, grads)\n",
    "        end\n",
    "        current_epoch += 1\n",
    "\n",
    "        # Compute a sample loss on a fixed subset\n",
    "        subset_x = train_x[:, :, 1:1000]\n",
    "        subset_y = train_y[:, 1:1000]\n",
    "        current_loss = loss_function(model, subset_x, subset_y)\n",
    "        println(\"Epoch $(current_epoch) complete. Sample training loss = $(current_loss)\")\n",
    "\n",
    "        # Save checkpoint at the given interval or on final epoch\n",
    "        if (current_epoch % checkpoint_interval == 0) || (epoch == epochs)\n",
    "            # Save CPU version of model parameters (optimizer state is not saved)\n",
    "            model_params = cpu.(Flux.params(model))\n",
    "            filename = \"$(checkpoint_prefix)$(current_epoch).bson\"\n",
    "            BSON.@save filename model_params current_epoch\n",
    "            println(\"Checkpoint saved for epoch $(current_epoch) in file $(filename)\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return model, current_epoch\n",
    "end\n",
    "\n",
    "\n",
    "# Load data (common to both training and resuming)\n",
    "train_x, train_y, test_x, test_y = load_data()\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# First phase: Train for 5 epochs and save checkpoint\n",
    "first_phase_epochs = 5\n",
    "model, current_epoch = train_model!(model, train_x, train_y; epochs=first_phase_epochs, checkpoint_interval=first_phase_epochs)\n",
    "checkpoint_file = \"checkpoint_epoch$(current_epoch).bson\"\n",
    "println(\"Checkpoint saved after training for $(current_epoch) epoch(s)\")\n",
    "\n",
    "# Second phase: Reload checkpoint and retrain\n",
    "println(\"Reloading checkpoint from file $(checkpoint_file)...\")\n",
    "BSON.@load checkpoint_file model_params current_epoch\n",
    "model_params = gpu.(model_params)\n",
    "Flux.loadparams!(model, model_params)\n",
    "println(\"Checkpoint reloaded. Resuming training from epoch $(current_epoch + 1)\")\n",
    "\n",
    "# Continue training until total of 10 epochs is reached\n",
    "total_epochs = 10\n",
    "remaining_epochs = total_epochs - current_epoch\n",
    "model, final_epoch = train_model!(model, train_x, train_y; epochs=remaining_epochs, checkpoint_interval=remaining_epochs)\n",
    "\n",
    "println(\"Training complete at epoch $(current_epoch + final_epoch)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Retrain.png\" alt=\"Model Checkpoint\" style=\"max-width:100%; width:1000px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed-Precision Training (Float16 on GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern GPUs can achieve higher throughput with lower precision (FP16/BFloat16) using tensor cores. \n",
    "\n",
    "Flux can leverage this by training with Float16 weights and gradients (mixed precision). This reduces memory usage and can significantly speed up math-intensive models on supported GPUs​. \n",
    "\n",
    "The main idea is to perform the forward pass and gradient computations in FP16 (for speed) while maintaining an `FP32 “master copy” of the weights. \n",
    "\n",
    "During updates, gradients are cast to FP32 so that the weight update 𝑤 = 𝑤 − 𝜂 × grad is computed in higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Problem with Pure FP16 Training\n",
    "When casting both data and model parameters entirely to FP16—especially when also normalizing inputs (e.g. dividing by 255)— risk running into severe precision issues. \n",
    "\n",
    "FP16 only provides about 3 decimal digits of precision, and small gradient values or scaling operations can cause underflow or rounding errors that accumulate during backpropagation. resulting in ```NaN``` or ```InF``` Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Convert model parameters to FP16\n",
    "to_float16(x) = x isa AbstractArray ? Float16.(x) : x\n",
    "model_f16 = fmap(to_float16, model) |> gpu\n",
    "\n",
    "# --- Mixed Precision Loss ---\n",
    "loss_function(x, y) = logitcrossentropy(Float32.(model_f16(x)), Float32.(y))\n",
    "\n",
    "# --- Optimizer and Training Setup ---\n",
    "opt = ADAM(0.0001)\n",
    "parameters = Flux.params(model_f16)\n",
    "train_loader = DataLoader((train_x_16, train_y_onehot_16), batchsize=128, shuffle=true)\n",
    "\n",
    "for epoch in 1:5\n",
    "    for (x_batch, y_batch) in train_loader\n",
    "        grads = gradient(() -> loss_function(x_batch, y_batch), parameters)\n",
    "        Flux.Optimise.update!(opt, parameters, grads)\n",
    "    end\n",
    "    # Loss monitoring\n",
    "    subset_x = train_x_16[:, :, :, 1:1000]\n",
    "    subset_y = train_y_onehot_16[:, 1:1000]\n",
    "    train_loss = loss_function(subset_x, subset_y)\n",
    "    println(\"Epoch $epoch complete. Sample training loss = $train_loss\")\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed Precision with an FP32 Master Copy\n",
    "\n",
    "1. Model Initialization:\n",
    "    - Define your model in FP32.\n",
    "\n",
    "2. Master Copy Creation:\n",
    "    - Create a deep‑copy of the FP32 parameters (the “master copy”) on the GPU.\n",
    "\n",
    "3. FP16 Computation Model:\n",
    "    - Convert the model to FP16 (using ```fmap(to_float16, ...)```) for on‑device computation.\n",
    "\n",
    "4. Loss Calculation:\n",
    "    - Do the forward pass in FP16, then cast outputs (and targets) to FP32 to compute a stable loss.\n",
    "\n",
    "5. Gradient Update:\n",
    "    - Compute gradients w.r.t. the FP16 model, cast gradients to FP32, update the FP32 master copy using ADAM, and then sync updated FP32 weights back to the FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: Dense, Chain, relu, flatten, gpu, fmap\n",
    "using Flux.Data: DataLoader\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using MLDatasets, CUDA, Plots\n",
    "using Flux.Optimise: ADAM\n",
    "using BSON  \n",
    "\n",
    "train_x_gpu, train_y_onehot_gpu, test_x_gpu, test_y_onehot_gpu = load_data()\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Convert and reshape training/testing data to Float16 and normalize to [0,1].\n",
    "train_x_16 = Float16.(reshape(train_x_gpu, 28, 28, 1, :)) \n",
    "test_x_16  = Float16.(reshape(test_x_gpu, 28, 28, 1, :)) \n",
    "train_y_onehot_16 = Float16.(train_y_onehot_gpu)\n",
    "test_y_onehot_16  = Float16.(test_y_onehot_gpu)\n",
    "\n",
    "# --- Model Definition (FP32) ---\n",
    "model = Chain(\n",
    "    flatten,                 # Flatten the input images\n",
    "    Dense(28*28, 128, relu),  # First dense layer with ReLU activation\n",
    "    Dense(128, 10)           # Output layer (logits for 10 classes)\n",
    ")\n",
    "\n",
    "# --- Create Master Copy (FP32) on GPU ---\n",
    "# This master copy will be updated using FP32 math.\n",
    "master_params = [deepcopy(p) |> gpu for p in Flux.params(model)]\n",
    "\n",
    "# --- Create FP16 Model for On-device Computation ---\n",
    "to_float16(x) = x isa AbstractArray ? Float16.(x) : x\n",
    "model_f16 = fmap(to_float16, model) |> gpu\n",
    "\n",
    "# --- Mixed Precision Loss ---\n",
    "# Compute forward pass in FP16 then cast outputs and targets to FP32 for loss computation.\n",
    "loss_function(x, y) = logitcrossentropy(Float32.(model_f16(x)), Float32.(y))\n",
    "\n",
    "# --- Optimizer Setup (for master FP32 parameters) ---\n",
    "opt = ADAM(0.0001)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader((train_x_16, train_y_onehot_16), batchsize=batch_size, shuffle=true)\n",
    "\n",
    "train_losses = Float32[]\n",
    "\n",
    "epochs = 10\n",
    "@btime begin\n",
    "for epoch in 1:epochs\n",
    "    for (x_batch, y_batch) in train_loader\n",
    "        # Compute gradients with respect to the FP16 model parameters.\n",
    "        gs = gradient(() -> loss_function(x_batch, y_batch), Flux.params(model_f16))\n",
    "        \n",
    "        # Update each parameter: convert the gradient to FP32 and update the master copy,\n",
    "        # then sync the master copy back to the FP16 model.\n",
    "        for (p16, p32) in zip(Flux.params(model_f16), master_params)\n",
    "            g = gs[p16]\n",
    "            g32 = Float32.(g)  # Convert gradient to FP32\n",
    "            Flux.Optimise.update!(opt, p32, g32)\n",
    "            p16 .= Float16.(p32)  # Sync updated FP32 master to FP16 model\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Evaluate loss on a subset for monitoring.\n",
    "    subset_x = train_x_16[:, :, :, 1:1000]\n",
    "    subset_y = train_y_onehot_16[:, 1:1000]\n",
    "    train_loss = loss_function(subset_x, subset_y)\n",
    "    \n",
    "    push!(train_losses, Float32(train_loss))\n",
    "    println(\"Epoch $epoch complete. Sample training loss = $train_loss\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "\n",
    "<div style=\"flex: 1; margin-right: 20px;\">\n",
    "  \n",
    "**Proper loss with learning FP32 Copy**\n",
    "\n",
    "| Epoch             | Loss                            |\n",
    "|-------------------|---------------------------------|\n",
    "| Epoch 1 complete. | Sample training loss = 0.504    |\n",
    "| Epoch 2 complete. | Sample training loss = 0.363    |\n",
    "| Epoch 3 complete. | Sample training loss = 0.311    |\n",
    "| Epoch 4 complete. | Sample training loss = 0.274    |\n",
    "| Epoch 5 complete. | Sample training loss = 0.253    |\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; margin-left: 30px;\">\n",
    "  \n",
    "**Unstable loss with learning Everything in FP16**\n",
    "\n",
    "| Epoch             | Loss                            |\n",
    "|-------------------|---------------------------------|\n",
    "| Epoch 1 complete. | Sample training loss = NaN      |\n",
    "| Epoch 2 complete. | Sample training loss = NaN      |\n",
    "| Epoch 3 complete. | Sample training loss = NaN      |\n",
    "| Epoch 4 complete. | Sample training loss = NaN      |\n",
    "| Epoch 5 complete. | Sample training loss = NaN      |\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider \n",
    "\n",
    "1. [The GPU memory is separate from CPU memory](https://computing.stat.berkeley.edu/tutorial-parallelization/parallel-python#6-using-the-gpu-via-pytorch), and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.\n",
    "\n",
    "2. On the other hand, GPUs can process these lower-precision operations (FP16) faster—sometimes leveraging specialized hardware like [Tensor Cores](https://datacrunch.io/blog/role-of-tensor-cores-in-parallel-computing-and-ai) (Processing unit in A100, H100 etc) —which results in a significant speed-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory & Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, BenchmarkTools\n",
    "\n",
    "x_cpu_16 = rand(Float16, 28, 28, 1, 1000)\n",
    "x_cpu_32 = rand(Float32, 28, 28, 1, 1000)\n",
    "\n",
    "# CPU -> GPU transfer time for Float16:\n",
    "@btime CUDA.@sync begin\n",
    "    x_gpu_16 = CuArray(x_cpu_16)\n",
    "    nothing\n",
    "end\n",
    "# 350.586 μs (10 allocations: 304 bytes)\n",
    "\n",
    "# CPU -> GPU transfer time for Float32\n",
    "@btime CUDA.@sync begin\n",
    "    x_gpu_32 = CuArray(x_cpu_32)\n",
    "    nothing\n",
    "end\n",
    "# 690.255 μs (10 allocations: 304 bytes)\n",
    "\n",
    "\n",
    "x_gpu_16 = CuArray(x_cpu_16)\n",
    "x_gpu_32 = CuArray(x_cpu_32)\n",
    "\n",
    "# GPU -> CPU transfer time for Float16\n",
    "@btime CUDA.@sync begin\n",
    "    out_16 = Array(x_gpu_16)\n",
    "    nothing\n",
    "end\n",
    "# 391.073 μs (8 allocations: 1.50 MiB)\n",
    "\n",
    "# GPU -> CPU transfer time for Float32\n",
    "@btime CUDA.@sync begin\n",
    "    out_32 = Array(x_gpu_32)\n",
    "    nothing\n",
    "end\n",
    "# 686.994 μs (8 allocations: 2.99 MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Core Speed Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from Note 8: GPUs\n",
    "n = 1000\n",
    "x16_gpu = CUDA.randn(Float16, n, n);\n",
    "y16_gpu = CUDA.randn(Float16, n, n);\n",
    "\n",
    "x32_gpu = CUDA.randn(n, n); \n",
    "y32_gpu = CUDA.randn(n, n);\n",
    "\n",
    "function matmult(x, y)\n",
    "    z = x * y;\n",
    "    return z\n",
    "end\n",
    "\n",
    "\n",
    "@btime CUDA.@sync z16_gpu = matmult(x16_gpu, y16_gpu);\n",
    "# 43.978 μs (50 allocations: 1.17 KiB)\n",
    "@btime CUDA.@sync z32_gpu = matmult(x32_gpu, y32_gpu);\n",
    "# 157.912 μs (50 allocations: 1.17 KiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Float32 values use 4 bytes per element versus 2 bytes per element for Float16. Hence, transferring a Float32 array 2x data, resulting 2x time.\n",
    "\n",
    "- 304 bytes is the overhead of CPU -> GPU transfer?\n",
    "\n",
    "- The allocation size (1.50 MiB vs. 2.99 MiB) GPU -> CPU corresponds to the actual data size in memory\n",
    "\n",
    "- Close to 4x speed up in matrix multiplication hint the contribution of tensor cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Scaling (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, especially when the loss is very small, you might apply a loss scaling factor to prevent gradient underflow in FP16.\n",
    "\n",
    "\n",
    "1. **Add a loss scaling factor** which multiply the loss before backpropagation\n",
    "2. **Modify loss function** to return both scaled loss (backpropagation) and unscaled loss (monitoring)  \n",
    "3. **Updatethe master model weight** in unscaled FP32 precision and copy it back to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mixed Precision Loss ---\n",
    "# Compute forward pass in FP16 then cast outputs and targets to FP32 for loss computation.\n",
    "loss_function(x, y) = logitcrossentropy(Float32.(model_f16(x)), Float32.(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mixed Precision Loss with Gradient Scaling ---\n",
    "\n",
    "loss_scale = Float32(128.0)  # loss scaling factor\n",
    "\n",
    "# Compute forward pass in FP16, apply loss scaling, and cast to FP32 for stable loss computation\n",
    "function loss_function(x, y)\n",
    "    outputs = model_f16(x) # Forward pass in FP16\n",
    "    loss = logitcrossentropy(Float32.(outputs), Float32.(y)) # FP32 loss computation\n",
    "    scaled_loss = loss * loss_scale # Scale loss \n",
    "    return scaled_loss, loss  # Return both scaled and unscaled loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:epochs\n",
    "    for (x_batch, y_batch) in train_loader\n",
    "        # Gradients w.r.t FP16 model Parms.\n",
    "        gs = gradient(() -> loss_function(x_batch, y_batch), Flux.params(model_f16))\n",
    "        \n",
    "\n",
    "        for (p16, p32) in zip(Flux.params(model_f16), master_params)\n",
    "            g = gs[p16]\n",
    "            g32 = Float32.(g)  # Convert gradient to FP32\n",
    "            Flux.Optimise.update!(opt, p32, g32)\n",
    "            p16 .= Float16.(p32)  # Sync updated FP32 master to FP16 model\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:epochs\n",
    "    for (x_batch, y_batch) in train_loader\n",
    "\n",
    "        # Use the scaled loss for backpropagation\n",
    "        gs = gradient(Flux.params(model_f16)) do\n",
    "            scaled_loss, _ = loss_function(x_batch, y_batch)\n",
    "            return scaled_loss\n",
    "        end\n",
    "        \n",
    "        for (p16, p32) in zip(Flux.params(model_f16), master_params)\n",
    "            g = gs[p16]\n",
    "            g32 = Float32.(g) # Convert gradient to FP32\n",
    "            \n",
    "            g32 ./= loss_scale # Unscale the gradient\n",
    "            \n",
    "            Flux.Optimise.update!(opt, p32, g32) # Sync updated FP32 master (unscaled gradient)\n",
    "            p16 .= Float16.(p32)\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [NVIDIA Mixed Precision Training Guidelines](https://www.dropbox.com/scl/fi/xk8bc4l68oyuxhbzsnrwh/Case-Mixed-Precision-Training-NeurIPS-Expo.pdf?rlkey=jee9fp9mkus03pinbb6xj5kqz&e=3&dl=0)\n",
    "\n",
    "- [Mixed Precision Training Conceptual Understanding](https://forums.fast.ai/t/mixed-precision-training/20720/1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Between Flux.jl and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Flux (Julia) | PyTorch (Python) |\n",
    "|---------|-------------|-----------------|\n",
    "| Language | Pure Julia, JIT compiled | Python, with C++ backend |\n",
    "| GPU Support | Native via CUDA.jl | Native via CUDA |\n",
    "| Ease of Use | Mathematical notation, hackable, and seamless integration with custom code | Flexible with extensive documentation and plug-and-play training loops |\n",
    "| Ecosystem | Growing; integrates with SciML and specialized libraries (e.g., DiffEqFlux) | Mature with a large community and extensive model/tool libraries |\n",
    "| Performance | High performance after JIT warm-up; may incur overhead on many small ops | Highly optimized with fused kernels and efficient memory management |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Kernels for Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adpated from Note 8: GPUs\n",
    "using CUDA, BenchmarkTools\n",
    "using Flux: flatten\n",
    "\n",
    "# Custom Kernel ReLU Activation\n",
    "function relu_kernel!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(x)\n",
    "        x[i] = x[i] > 0f0 ? x[i] : 0f0\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "function relu_activation!(x) # Helper function\n",
    "    n = length(x)\n",
    "    threads = min(n, 1024)\n",
    "    blocks = cld(n, threads)\n",
    "    @cuda threads=threads blocks=blocks relu_kernel!(x)\n",
    "    return\n",
    "end\n",
    "\n",
    "# Baseline: Broadcasting-based ReLU\n",
    "function relu_builtin(x)\n",
    "    return max.(x, 0f0)\n",
    "end\n",
    "\n",
    "x = cu(randn(Float32, 28, 28, 1, 128));\n",
    "\n",
    "# Custom Kernel\n",
    "@btime CUDA.@sync begin\n",
    "    relu_activation!(x)\n",
    "    nothing\n",
    "end\n",
    "# 14.538 μs (18 allocations: 560 bytes)\n",
    "\n",
    "# Baseline\n",
    "@btime CUDA.@sync begin\n",
    "    y_builtin = relu_builtin($x)\n",
    "    nothing\n",
    "end\n",
    "# 26.857 μs (70 allocations: 1.72 KiB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
